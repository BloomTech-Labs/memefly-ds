{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memefly Image Captioning Word Level\n",
    "\n",
    "Inspired by [Show, Attend and Tell: Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044), [Dank Learning: Generating Memes Using Deep Neural Networks](https://arxiv.org/abs/1806.04510), and [CS231n Assignment 3](http://cs231n.github.io/assignments2019/assignment3/). \n",
    "\n",
    "Code references [Image captioning with visual attention](https://www.tensorflow.org/tutorials/text/image_captioning), [Text generation with an RNN](https://www.tensorflow.org/tutorials/text/text_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../datasets'))\n",
    "sys.path.append(os.path.abspath('../weights'))\n",
    "\n",
    "import pathlib\n",
    "import time\n",
    "import pickle\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import ipykernel\n",
    "import requests\n",
    "import shutil\n",
    "from typing import List, Dict, Tuple, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout, GRU, Add, add, Attention, RepeatVector, AdditiveAttention\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import wandb\n",
    "from wandb.keras import WandbCallback\n",
    "\n",
    "os.environ['WANDB_NOTEBOOK_NAME'] = '10_meme_word_gen_model_3.2.2'\n",
    "np.random.seed(45)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    DATA_VERSION = 'v2'\n",
    "    MODEL_TYPE = 'word'\n",
    "    TIMESTAMP = time.strftime('%Y%m%d%H%M')\n",
    "    TOKENIZER = f'../weights/memefly-{MODEL_TYPE}-data-{DATA_VERSION}-tokenizer.pkl'\n",
    "    IMAGE_MODEL_FILENAME = \"../weights/inceptionv3_embeddings.h5\"\n",
    "\n",
    "INPUT_JSON_FILE = '../datasets/combined_data.json'\n",
    "DESCRIPTION_FILE = f'../datasets/memefly-{Config.DATA_VERSION}-descriptions.txt'\n",
    "IMG_FEATURES_PKL = f'../datasets/memefly-{Config.DATA_VERSION}-features.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeflyDataset:\n",
    "    def __init__(self, *, input_json_file: str, img_model: tf.keras.Model, description_file: str, img_features_pkl: str):\n",
    "        self.json_data = self.__load_json(input_json_file)\n",
    "        self.description_file = description_file\n",
    "        self.img_features_pkl = img_features_pkl\n",
    "        self.img_model = tf.keras.models.load_model(img_model, compile=False)\n",
    "        self.tokenizer = None\n",
    "        self.max_length = None\n",
    "        self.vocab_size = None\n",
    "        self.text_data = None\n",
    "        self.img_data = None\n",
    "\n",
    "    def __load_json(self, path: str):\n",
    "        \"\"\" Loads json file \"\"\"\n",
    "        try:\n",
    "            with open(path) as robj:\n",
    "                data = json.load(robj)\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            raise e\n",
    "        \n",
    "    def preprocess_text(self):\n",
    "        \"\"\"\n",
    "        Preprocess input_json and save to instance attributes\n",
    "        \n",
    "        Generates:\n",
    "        ========\n",
    "        description_file: meme_name meme_text file, for sanity checking and debugging\n",
    "        tokenizer: tf.keras tokenizer\n",
    "        vocab_size: size of the tokenizer, int\n",
    "        max_length: maximum length of meme text, int\n",
    "        text_data: list of [meme_name, meme_text] pairs\n",
    "        \"\"\"\n",
    "        print(\"Preprocessing text ...\")\n",
    "        corpus = []\n",
    "        meme_data = []\n",
    "        with open(self.description_file, 'w') as outfile:\n",
    "            for row in iter(self.json_data):\n",
    "                meme_name = row[\"meme_name\"]\n",
    "                for meme_text in row[\"meme_text\"]:\n",
    "                    meme_text = f\"startseq {meme_text} endseq\\n\"\n",
    "                    #text = f\"{meme_name} startseq {meme_text} endseq\\n\"\n",
    "                    corpus.append(meme_text.rstrip())#.split(' '))\n",
    "                    meme_data.append([meme_name, meme_text.rstrip()])\n",
    "                    outfile.write(f\"{meme_name} {meme_text}\")#text)\n",
    "        \n",
    "        tokenizer = Tokenizer(lower=True)\n",
    "        tokenizer.fit_on_texts(corpus)\n",
    "        pickle.dump(tokenizer, open(Config.TOKENIZER, 'wb'))\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.vocab_size = len(tokenizer.word_index) + 1\n",
    "        self.text_data = meme_data\n",
    "        self.max_length = len(max([item[1] for item in meme_data], key=len))\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def preprocess_img(self):\n",
    "        \"\"\"\n",
    "        Preprocess input_json and save to instance attributes\n",
    "        \n",
    "        Generates:\n",
    "        ========\n",
    "        images files: downloaded image file given the urls\n",
    "        img_features_pkl: pickled dictionary of {meme_name: img_vec file}\n",
    "        img_data: dictionary of {meme_name: img_vec file}\n",
    "        \"\"\"\n",
    "        print(\"Preprocessing images ...\")\n",
    "        img_urls = [item['meme_url'] for item in self.json_data]\n",
    "        meme_names = [item['meme_name'] for item in self.json_data]\n",
    "        self.__download_images(img_urls, meme_names)\n",
    "        self.img_data = self.__extract_features(meme_names)\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def __download_images(self, url_list: List, meme_name: List):\n",
    "        \"\"\" Download meme images from 'meme_url', skip if already exists \"\"\"\n",
    "        print(\"Downloading images ...\")\n",
    "        count = 0\n",
    "        for i in tqdm(range(len(url_list))):\n",
    "            filename = f\"../datasets/images/{meme_name[i]}.jpg\"\n",
    "            if not pathlib.Path(filename).exists():\n",
    "                r = requests.get(url_list[i], \n",
    "                                 stream=True,\n",
    "                                 #headers={'User-agent': 'Mozilla/5.0'}\n",
    "                                )\n",
    "                if r.status_code == 200:\n",
    "                    count += 1\n",
    "\n",
    "                    with open(filename, 'wb') as f:\n",
    "                        r.raw.decode_content = True\n",
    "                        shutil.copyfileobj(r.raw, f)\n",
    "        if count == len(url_list):\n",
    "            print(\"all images in url_list downloaded\")\n",
    "            \n",
    "        pass\n",
    "    \n",
    "    def __extract_features(self, meme_name: list) -> dict:\n",
    "        \"\"\"\n",
    "        Takes a preloaded Tensorflow Keras InceptionV3 Model with embeddings and a list of images\n",
    "        and return a dict with keys: image_name w/o the .jpg and the values: image embeddings extracted\n",
    "        using InceptionV3 with global average pooling layer and pretrained imagenet weights.\n",
    "        \"\"\"\n",
    "        print(\"Creating image embedding vectors ...\")\n",
    "        features = dict()\n",
    "        for img_file in tqdm(meme_name):\n",
    "            filename = f\"../datasets/images/{img_file}.jpg\"\n",
    "            img = load_img(filename, target_size=(299, 299))\n",
    "            img = img_to_array(img)\n",
    "            img = img.reshape((1, img.shape[0], img.shape[1], img.shape[2]))\n",
    "            img = preprocess_input(img)\n",
    "            feature = self.img_model.predict(img, verbose=0)\n",
    "            features[img_file] = feature\n",
    "            \n",
    "            pickle.dump(features, open(self.img_features_pkl, 'wb'))\n",
    "            \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing text ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 108/108 [00:00<00:00, 5999.64it/s]\n",
      "  0%|                                                                                          | 0/108 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing images ...\n",
      "Downloading images ...\n",
      "Creating image embedding vectors ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 108/108 [00:14<00:00,  7.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full data: 20443\n",
      "memes images: 108\n",
      "Vocab size: 17240\n",
      "Max meme length: 150\n",
      "\n",
      "19420 1023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = MemeflyDataset(input_json_file=INPUT_JSON_FILE,\n",
    "                         img_model=Config.IMAGE_MODEL_FILENAME, \n",
    "                         description_file=DESCRIPTION_FILE, \n",
    "                         img_features_pkl=IMG_FEATURES_PKL)\n",
    "dataset.preprocess_text()\n",
    "dataset.preprocess_img()\n",
    "\n",
    "meme_dataset = dataset.text_data\n",
    "MEME_IMG_VEC = dataset.img_data\n",
    "VOCAB_SIZE = dataset.vocab_size\n",
    "MAX_LENGTH = dataset.max_length\n",
    "TOKENIZER = dataset.tokenizer\n",
    "print(f\"Full data: {len(meme_dataset)}\\nmemes images: {len(MEME_IMG_VEC)}\\nVocab size: {VOCAB_SIZE}\\nMax meme length: {MAX_LENGTH}\\n\")\n",
    "\n",
    "train_dataset, val_dataset = train_test_split(meme_dataset, test_size=0.05)\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemeDataGenerator(tf.keras.utils.Sequence):\n",
    "    \"\"\"\n",
    "    An iterable that returns [batch_size, (images embeddigns, [unrolled input text sequences, text target])].\n",
    "    Instead of batching over images, we choose to batch over [image, description] pairs because unlike typical\n",
    "    image captioning tasks that has 3-5 texts per image, we have 180-200 texts per image. Batching over images\n",
    "    in our case significantly boosted memory cost and we could only batch 1-2 images using AWS p2.xLarge or\n",
    "    p3\n",
    "    \n",
    "    This class inherets from tf.keras.utils.Sequences to avoid data redundancy and syncing error.\n",
    "    https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence\n",
    "    https://keras.io/utils/#sequence\n",
    "    \n",
    "    dataset: [meme name, meme text] pairs\n",
    "    shuffle: If True, shuffles the samples before every epoch\n",
    "    batch_size: How many images to return in each call\n",
    "    \n",
    "    INPUT:\n",
    "    ========\n",
    "    - dataset: list of meme_name and meme_text pairs. [[meme_name, meme_text], [...], ...]\n",
    "    - img_embds: a pickled dictionary of {meme_name: image embeddings}\n",
    "    - tokenizer: tf.keras.preprocessing.text.Tokenizer\n",
    "    - batch_size: batch size\n",
    "    - max_length: maximum length of words\n",
    "    - vocab_size: size of the vocaburaries.\n",
    "    - shuffle: if True, shuffles the dataset between every epoch\n",
    "    \n",
    "    OUTPUT:\n",
    "    =======\n",
    "    - outputs list: Usually empty in regular training. But if detection_targets\n",
    "      is True then the outputs list contains target class_ids, bbox deltas,\n",
    "      and masks.\n",
    "    \"\"\"\n",
    "    def __init__(self, *, dataset, img_embds, tokenizer, batch_size: int, max_length: int, vocab_size: int, shuffle=True):\n",
    "        self.dataset = dataset\n",
    "        self.img_embds = img_embds\n",
    "        self.tokenizer = tokenizer\n",
    "        self.batch_size = batch_size\n",
    "        self.max_length = max_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.shuffle = shuffle\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\" Number of batches in the Sequence \"\"\"\n",
    "        return int(np.floor(len(self.dataset) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Generate one batch of data. One element in a batch is a pair of meme_name, meme_text.\n",
    "        Dataset is indexed using 'indexes' and 'indexes' will be shuffled every epoch if shuffle is True.\n",
    "        \"\"\"\n",
    "        indexes = self.indexes[idx*self.batch_size:(idx+1)*self.batch_size]       \n",
    "        current_data = [self.dataset[i] for i in indexes]\n",
    "        in_img, in_seq, out_word = self.__generate_data(current_data)\n",
    "        \n",
    "        return [in_img, in_seq], out_word\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        \"\"\" Method called at between every epoch \"\"\"\n",
    "        self.indexes = np.arange(len(self.dataset))\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "        pass\n",
    "    \n",
    "    def __generate_data(self, data_batch):\n",
    "        \"\"\"\n",
    "        Loop through the batch of data list and generate unrolled sequences of each list of data\n",
    "        \"\"\"\n",
    "        X1, X2, y = list(), list(), list()\n",
    "        for data in data_batch:\n",
    "            img_embd = self.img_embds[data[0]][0]\n",
    "            X1_tmp, X2_tmp, y_tmp = self.__create_sequence(img_embd, data[1])\n",
    "            # append creates list of lists. extend doesnt.\n",
    "            X1.extend(X1_tmp)\n",
    "            X2.extend(X2_tmp)\n",
    "            y.extend(y_tmp)\n",
    "        \n",
    "        return np.array(X1), np.array(X2), np.array(y)\n",
    "    \n",
    "    def __create_sequence(self, image, meme_text):\n",
    "        \"\"\"\n",
    "        Create one sequence of images, input sequences and output text for a single meme_text, e.g., \n",
    "        \n",
    "        img_vec   input                               output\n",
    "        ========  ========                            ========\n",
    "        IMAGE_VEC startseq                            hi\n",
    "        IMAGE_VEC startseq hi                         this\n",
    "        IMAGE_VEC startseq hi this                    is\n",
    "        IMAGE_VEC startseq hi this is                 not\n",
    "        IMAGE_VEC startseq hi this is not             fun\n",
    "        IMAGE_VEC startseq hi this is not fun         endseq\n",
    "        \n",
    "        Tokenized sequences will be padded from the front, keras default. The output word will be\n",
    "        one hot encoded w/ keras' to_categorical, and to save memory size, we cast it to float16\n",
    "        # https://stackoverflow.com/questions/42943291/what-does-keras-io-preprocessing-sequence-pad-sequences-do\n",
    "                \n",
    "        INPUT:\n",
    "        ========\n",
    "        image:      image vectors\n",
    "        meme_text:  text to be unrolled into max length length of sequences\n",
    "        tokenizer:  tokenizer used to convert words to numbers\n",
    "\n",
    "        OUTPUT:\n",
    "        ========\n",
    "        X1:         image vector, list\n",
    "        X2:         tokenized sequences, padded to max length, list\n",
    "        y:          next texts, target, list\n",
    "        \"\"\"\n",
    "        X1, X2, y = list(), list(), list()\n",
    "        \n",
    "        seq = self.tokenizer.texts_to_sequences([meme_text])[0]\n",
    "        for i in range(1, len(seq)):\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            in_seq = pad_sequences([in_seq], maxlen=self.max_length)[0]\n",
    "            out_seq = to_categorical([out_seq], num_classes=self.vocab_size, dtype='float16')[0]\n",
    "\n",
    "            X1.append(image)\n",
    "            X2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "            \n",
    "        return X1, X2, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_captioning_model(*, vocab_size: int, maxlen: int, embedding_dim: int, rnn_units: int, batch_size: int) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Injecting image embedding using par-inject method (3) as described in the following paper.\n",
    "    [Where to put the Image in an Image CaptionGenerator](https://arxiv.org/abs/1703.09137)\n",
    "    \n",
    "    Par-inject was used as [Neural Image Caption Generation with Visual Attention](https://arxiv.org/abs/1502.03044)\n",
    "    \"\"\"\n",
    "    \n",
    "    img_emb_input = Input(shape=(2048,), name=\"image_input\")\n",
    "    x1 = Dropout(0.5)(img_emb_input)\n",
    "    x1 = Dense(embedding_dim, activation='relu', name='image_dense')(x1)\n",
    "    x1 = RepeatVector(maxlen)(x1)\n",
    "\n",
    "    tokenized_text_input = Input(shape=(maxlen,), name='text_input')\n",
    "    x2 = Embedding(vocab_size, embedding_dim, mask_zero=True, batch_input_shape=[batch_size, None], name='text_embedding')(tokenized_text_input)\n",
    "    \n",
    "    decoder = Concatenate(name='image_text_concat')([x1, x2]) #add([x1, x2])\n",
    "    decoder = GRU(rnn_units, name='GRU')(decoder)\n",
    "    decoder = Dense(256, activation='relu', name='last_dense')(decoder)\n",
    "    outputs = Dense(vocab_size, activation='softmax', name='output')(decoder)\n",
    "\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[img_emb_input, tokenized_text_input], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "553/554 [============================>.] - ETA: 1s - loss: 6.9877\n",
      "Epoch 00001: saving model to ../weights/ckpt/memefly-word-150-201912062354-01-6.56.h5\n",
      "554/554 [==============================] - 693s 1s/step - loss: 6.9874 - val_loss: 6.5648\n",
      "Epoch 2/2\n",
      "553/554 [============================>.] - ETA: 1s - loss: 6.1801\n",
      "Epoch 00002: saving model to ../weights/ckpt/memefly-word-150-201912062354-02-5.94.h5\n",
      "554/554 [==============================] - 697s 1s/step - loss: 6.1799 - val_loss: 5.9438\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c5b2f1f1d0>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128 # p3.2xlarge\n",
    "wandb.init(config={\"hyper\": \"parameter\"}, project=\"\")\n",
    "\n",
    "train_datagen = MemeDataGenerator(dataset=train_dataset, \n",
    "                                  img_embds=MEME_IMG_VEC,\n",
    "                                  tokenizer=TOKENIZER,\n",
    "                                  batch_size=BATCH_SIZE,\n",
    "                                  max_length=MAX_LENGTH,\n",
    "                                  vocab_size=VOCAB_SIZE)\n",
    "\n",
    "val_datagen = MemeDataGenerator(dataset=val_dataset,\n",
    "                                img_embds=MEME_IMG_VEC,\n",
    "                                tokenizer=TOKENIZER,\n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                max_length=MAX_LENGTH,\n",
    "                                vocab_size=VOCAB_SIZE)\n",
    "\n",
    "model = image_captioning_model(vocab_size=VOCAB_SIZE, \n",
    "                               maxlen=MAX_LENGTH, \n",
    "                               embedding_dim=256, \n",
    "                               rnn_units=256, \n",
    "                               batch_size=BATCH_SIZE)\n",
    "model.summary()\n",
    "\n",
    "filepath = f\"../weights/ckpt/memefly-{Config.MODEL_TYPE}-{MAX_LENGTH}-{Config.TIMESTAMP}\"+\"-{epoch:02d}-{val_loss:.2f}.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, \n",
    "                             verbose=1, \n",
    "                             save_weights_only=False, \n",
    "                             save_best_only=False)\n",
    "\n",
    "model.fit_generator(train_datagen, \n",
    "                    epochs=EPOCHS, \n",
    "                    verbose=1, \n",
    "                    validation_data=val_datagen,\n",
    "                    callbacks=[checkpoint])\n",
    "                    #callbacks=[WandbCallback(), checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
